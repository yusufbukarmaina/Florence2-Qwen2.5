# Configuration file for Beaker Volume Prediction Training

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
[dataset]
# Your HuggingFace dataset name (e.g., username/beaker-dataset)
dataset_name = YOUR_HF_USERNAME/beaker-dataset

# Dataset splits (should sum to 1.0)
train_split = 0.70
validation_split = 0.15
test_split = 0.15


# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
[model]
# Model type: qwen2vl or florence2
model_type = qwen2vl

# Base model names (do not change unless using different base models)
qwen2vl_base = Qwen/Qwen2-VL-2B-Instruct
florence2_base = microsoft/Florence-2-base


# ============================================================================
# LORA CONFIGURATION
# ============================================================================
[lora]
# LoRA rank
rank = 16

# LoRA alpha
alpha = 32

# LoRA dropout
dropout = 0.05

# Target modules for Qwen2-VL
qwen2vl_target_modules = q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# Target modules for Florence2
florence2_target_modules = q_proj,k_proj,v_proj,o_proj


# ============================================================================
# TRAINING HYPERPARAMETERS
# ============================================================================
[training]
# Number of training epochs
epochs = 10

# Batch size per device (reduce if OOM)
batch_size = 4

# Gradient accumulation steps (increase if reducing batch_size)
gradient_accumulation_steps = 4

# Learning rate
learning_rate = 2e-5

# Warmup steps
warmup_steps = 100

# Maximum gradient norm for clipping
max_grad_norm = 1.0

# Weight decay
weight_decay = 0.01

# Learning rate scheduler type
lr_scheduler_type = cosine

# Save strategy: steps or epoch
save_strategy = steps

# Save every N steps
save_steps = 100

# Evaluation strategy: steps or epoch
eval_strategy = steps

# Evaluate every N steps
eval_steps = 100

# Maximum number of checkpoints to keep
save_total_limit = 3

# Early stopping patience (number of eval steps)
early_stopping_patience = 3


# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================
[output]
# Output directory for Qwen2-VL
qwen_output_dir = ./output_qwen

# Output directory for Florence2
florence_output_dir = ./output_florence

# Logging directory
logging_dir = ./logs


# ============================================================================
# HUGGINGFACE HUB CONFIGURATION
# ============================================================================
[huggingface]
# Your HuggingFace username
username = YOUR_HF_USERNAME

# HuggingFace token (get from https://huggingface.co/settings/tokens)
token = YOUR_HF_TOKEN

# Repository name for Qwen2-VL model
qwen_repo_name = YOUR_HF_USERNAME/beaker-qwen2vl

# Repository name for Florence2 model
florence_repo_name = YOUR_HF_USERNAME/beaker-florence2

# Upload to HuggingFace after training
upload_to_hf = true


# ============================================================================
# WEIGHTS & BIASES CONFIGURATION (Optional)
# ============================================================================
[wandb]
# Weights & Biases token (optional, leave empty if not using)
token = YOUR_WANDB_TOKEN

# W&B project name
project_name = beaker-volume-prediction

# W&B run name (will be auto-generated with timestamp if empty)
run_name = 

# Enable W&B logging
enabled = true


# ============================================================================
# JARVISLAB CONFIGURATION
# ============================================================================
[jarvislab]
# JarvisLab workspace directory
workspace_dir = /workspace/beaker-volume-prediction

# GPU type (for reference only)
gpu_type = A100

# Number of GPUs (for multi-GPU training)
num_gpus = 1


# ============================================================================
# INFERENCE CONFIGURATION
# ============================================================================
[inference]
# Maximum new tokens for generation
max_new_tokens = 50

# Number of beams for beam search
num_beams = 3

# Temperature for sampling (0.0 for greedy)
temperature = 0.0

# Use sampling (true/false)
do_sample = false


# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
[evaluation]
# Metrics to compute: mae, rmse, r2
metrics = mae,rmse,r2

# Create visualizations during evaluation
create_plots = true

# Save predictions to file
save_predictions = true
